{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import datetime\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 25\n",
    "\n",
    "def cubic_sentences(file_dir):\n",
    "    phoneme_sentences = []\n",
    "    for filename in file_dir:\n",
    "        df = np.array(pd.read_csv(filename, header=None), dtype='float32')\n",
    "\n",
    "        check = 0\n",
    "        tmp = []\n",
    "        for i in range(df.shape[0]):\n",
    "            if df[i,1] == 10000:\n",
    "                if check == 0:\n",
    "                    data_per_window = tmp\n",
    "                    tmp = []\n",
    "                    check = 1\n",
    "                else:\n",
    "                    data_per_window = np.dstack((data_per_window,tmp))\n",
    "                    tmp = []\n",
    "            else:\n",
    "                if len(tmp) == 0:\n",
    "                    tmp = df[i,:]\n",
    "                else:            \n",
    "                    tmp = np.vstack((tmp,df[i,:]))\n",
    "        \n",
    "        data_per_window = np.moveaxis(data_per_window, -1, 0)\n",
    "        phoneme_sentences.append(data_per_window)\n",
    "        \n",
    "    return phoneme_sentences # list타입, 각 element에는 cube형태의 sentence샘플, sentence샘플의 shape: (sentence_length, window_size, feature_dim)\n",
    "\n",
    "def normalize_sentences(left_sentences, right_sentences):\n",
    "    sentences = left_sentences + right_sentences\n",
    "    count = 0\n",
    "    for i, stc in enumerate(sentences):\n",
    "        for j in range(stc.shape[0]):\n",
    "            each_timestep = stc[j,:,:]\n",
    "            if count==0:\n",
    "                stacked_sentences = each_timestep\n",
    "            else:\n",
    "                stacked_sentences = np.vstack((stacked_sentences, each_timestep))\n",
    "            \n",
    "            count = count + 1\n",
    "        \n",
    "    sentence_mean = np.mean(stacked_sentences, axis=0) # (1,feature_dim)\n",
    "    sentence_std = np.std(stacked_sentences, axis=0) # (1,feature_dim)\n",
    "    for i, stc in enumerate(sentences):\n",
    "        sentences[i] = (stc - sentence_mean)/sentence_std\n",
    "        \n",
    "    output_left_sentences = sentences[:len(left_sentences)]\n",
    "    output_right_sentences = sentences[len(left_sentences):]\n",
    "        \n",
    "    return output_left_sentences, output_right_sentences, sentence_mean, sentence_std\n",
    "\n",
    "def normalize_dict(phoneme_dict, feature_mean, feature_std):\n",
    "    output_phoneme_dict = (phoneme_dict - feature_mean)/feature_std\n",
    "    return output_phoneme_dict\n",
    "    \n",
    "def zero_padding_sentences(sentences,max_stc_len):\n",
    "    for i in range(len(sentences)):\n",
    "        tmp = sentences[i]\n",
    "        padded = np.vstack((tmp,np.zeros((max_stc_len-len(tmp),tmp.shape[1],tmp.shape[2]))))\n",
    "        sentences[i] = padded\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def convert_to_4D(sentences,max_stc_len):\n",
    "    dataset = np.empty((len(sentences),max_stc_len,sentences[0].shape[1],sentences[0].shape[2]))\n",
    "    for i in range(dataset.shape[0]):\n",
    "        dataset[i,:,:,:] = sentences[i]\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "# Get list of all the sentences\n",
    "HSL_path = r\"D:\\\\MinHyuk\\\\Hand Sign Recognition\\\\2021_SLT_prj\\\\DATA\\\\feature\\\\HSL\"\n",
    "HSR_path = r\"D:\\\\MinHyuk\\\\Hand Sign Recognition\\\\2021_SLT_prj\\\\DATA\\\\feature\\\\HSR\"\n",
    "LL_path = r\"D:\\\\MinHyuk\\\\Hand Sign Recognition\\\\2021_SLT_prj\\\\DATA\\\\feature\\\\LL\"\n",
    "LR_path = r\"D:\\\\MinHyuk\\\\Hand Sign Recognition\\\\2021_SLT_prj\\\\DATA\\\\feature\\\\LR\"\n",
    "ML_path = r\"D:\\\\MinHyuk\\\\Hand Sign Recognition\\\\2021_SLT_prj\\\\DATA\\\\feature\\\\ML\"\n",
    "MR_path = r\"D:\\\\MinHyuk\\\\Hand Sign Recognition\\\\2021_SLT_prj\\\\DATA\\\\feature\\\\MR\"\n",
    "\n",
    "HSL_all_files = glob(HSL_path+\"\\\\*.csv\")\n",
    "HSR_all_files = glob(HSR_path+\"\\\\*.csv\")\n",
    "LL_all_files = glob(LL_path+\"\\\\*.csv\")\n",
    "LR_all_files = glob(LR_path+\"\\\\*.csv\")\n",
    "ML_all_files = glob(ML_path+\"\\\\*.csv\")\n",
    "MR_all_files = glob(MR_path+\"\\\\*.csv\")\n",
    "\n",
    "HSL_sentences = cubic_sentences(HSL_all_files)\n",
    "HSR_sentences = cubic_sentences(HSR_all_files)\n",
    "LL_sentences = cubic_sentences(LL_all_files)\n",
    "LR_sentences = cubic_sentences(LR_all_files)\n",
    "ML_sentences = cubic_sentences(ML_all_files)\n",
    "MR_sentences = cubic_sentences(MR_all_files)\n",
    "\n",
    "# HSL_sentences, HSR_sentences, HS_mean, HS_std = normalize_sentences(HSL_sentences, HSR_sentences)\n",
    "# LL_sentences, LR_sentences, L_mean, L_std = normalize_sentences(LL_sentences, LR_sentences)\n",
    "# ML_sentences, MR_sentences, M_mean, M_std = normalize_sentences(ML_sentences, MR_sentences)\n",
    "\n",
    "HS_path = r\"D:\\\\MinHyuk\\\\Hand Sign Recognition\\\\2021_SLT_prj\\\\DATA\\\\phoneme_dict\\\\Handshape\"\n",
    "HS_file = glob(HS_path+\"\\\\\"+\"HS_normalized.csv\")\n",
    "HS_dict = np.array(pd.read_csv(HS_file[0], header=None), dtype='float32')\n",
    "# HS_dict = normalize_dict(HS_dict, HS_mean, HS_std)\n",
    "HS_dict = tf.convert_to_tensor(np.transpose(HS_dict))\n",
    "\n",
    "L_path = r\"D:\\\\MinHyuk\\\\Hand Sign Recognition\\\\2021_SLT_prj\\\\DATA\\\\phoneme_dict\\\\Location\"\n",
    "L_file = glob(L_path+\"\\\\\"+\"L_normalized.csv\")\n",
    "L_dict = np.array(pd.read_csv(L_file[0], header=None), dtype='float32')\n",
    "# L_dict = normalize_dict(L_dict, L_mean, L_std)\n",
    "L_dict = tf.convert_to_tensor(np.transpose(L_dict))\n",
    "\n",
    "M_path = r\"D:\\\\MinHyuk\\\\Hand Sign Recognition\\\\2021_SLT_prj\\\\DATA\\\\phoneme_dict\\\\Movement\"\n",
    "M_file = glob(M_path+\"\\\\\"+\"M_normalized.csv\")\n",
    "M_dict = np.array(pd.read_csv(M_file[0], header=None), dtype='float32')\n",
    "# M_dict = normalize_dict(M_dict, M_mean, M_std)\n",
    "M_dict = tf.convert_to_tensor(np.transpose(M_dict))\n",
    "\n",
    "# 문장 샘플 중 가장 긴 길이 찾기.\n",
    "# max_stc_len = max([len(stc) for stc in HSL_sentences])\n",
    "# HSL_sentences = zero_padding_sentences(HSL_sentences,max_stc_len)\n",
    "# HSR_sentences = zero_padding_sentences(HSR_sentences,max_stc_len)\n",
    "# LL_sentences = zero_padding_sentences(LL_sentences,max_stc_len)\n",
    "# LR_sentences = zero_padding_sentences(LR_sentences,max_stc_len)\n",
    "# ML_sentences = zero_padding_sentences(ML_sentences,max_stc_len)\n",
    "# MR_sentences = zero_padding_sentences(MR_sentences,max_stc_len)\n",
    "\n",
    "# 각 phoneme별 sentences를 4차원 numpy array로 변환. (num of sentence sample, timestep, window size, feature dim)\n",
    "# HSL_dataset = convert_to_4D(HSL_sentences, max_stc_len)\n",
    "# HSR_dataset = convert_to_4D(HSR_sentences, max_stc_len)\n",
    "# LL_dataset = convert_to_4D(LL_sentences, max_stc_len)\n",
    "# LR_dataset = convert_to_4D(LR_sentences, max_stc_len)\n",
    "# ML_dataset = convert_to_4D(ML_sentences, max_stc_len)\n",
    "# MR_dataset = convert_to_4D(MR_sentences, max_stc_len)\n",
    "\n",
    "stc_seq = [[0,1,2],[3,4,5],[6,7,2],[8,5,9],[10,8,11],[4,2,9],[12,13,8,5],\n",
    "          [0,14,15,16],[0,13,17,18],[19,13,1,20],[3,13,1,5],[15,8,6,21],\n",
    "          [15,1,22,5],[10,7,0,18],[11,14,23,24,20],[0,7,23,22,5],[15,7,23,24,21],\n",
    "          [10,8,23,24,21]] # 0부터 인덱싱\n",
    "num_words = 25\n",
    "labels = []\n",
    "for filename in HSL_all_files:\n",
    "    temp = filename.split(os.path.sep)[-1].split(\".csv\")[0]\n",
    "    temp = temp.split('-')[1]\n",
    "    temp = temp[8:]\n",
    "    labels.append(np.array(stc_seq[int(temp)-1]))\n",
    "labels = tf.ragged.constant(labels)\n",
    "#labels = np.array(labels)\n",
    "#labels = tf.convert_to_tensor(labels, dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSL_dataset = tf.ragged.constant(HSL_sentences, inner_shape=(window_size,10))\n",
    "# HSL_dataset = tf.RaggedTensor.from_tensor(tf.random.shuffle(HSL_dataset.to_tensor()))\n",
    "HSR_dataset = tf.ragged.constant(HSR_sentences, inner_shape=(window_size,10))\n",
    "# HSR_dataset = tf.RaggedTensor.from_tensor(tf.random.shuffle(HSR_dataset.to_tensor()))\n",
    "LL_dataset = tf.ragged.constant(LL_sentences, inner_shape=(window_size,12))\n",
    "# LL_dataset = tf.RaggedTensor.from_tensor(tf.random.shuffle(LL_dataset.to_tensor()))\n",
    "LR_dataset = tf.ragged.constant(LR_sentences, inner_shape=(window_size,12))\n",
    "# LR_dataset = tf.RaggedTensor.from_tensor(tf.random.shuffle(LR_dataset.to_tensor()))\n",
    "ML_dataset = tf.ragged.constant(ML_sentences, inner_shape=(1,window_size+3))\n",
    "# ML_dataset = tf.RaggedTensor.from_tensor(tf.random.shuffle(ML_dataset.to_tensor()))\n",
    "MR_dataset = tf.ragged.constant(MR_sentences, inner_shape=(1,window_size+3))\n",
    "# MR_dataset = tf.RaggedTensor.from_tensor(tf.random.shuffle(MR_dataset.to_tensor()))\n",
    "\n",
    "indices = tf.range(start=0, limit=len(HSL_sentences))\n",
    "shuffled_indices = tf.random.shuffle(indices)\n",
    "\n",
    "HSL_dataset = tf.gather(HSL_dataset, shuffled_indices)\n",
    "HSR_dataset = tf.gather(HSR_dataset, shuffled_indices)\n",
    "LL_dataset = tf.gather(LL_dataset, shuffled_indices)\n",
    "LR_dataset = tf.gather(LR_dataset, shuffled_indices)\n",
    "ML_dataset = tf.gather(ML_dataset, shuffled_indices)\n",
    "MR_dataset = tf.gather(MR_dataset, shuffled_indices)\n",
    "labels = tf.gather(labels, shuffled_indices)\n",
    "\n",
    "def encode_dict(HSL, HSR, LL, LR, ML, MR, label):\n",
    "    return{\"HSL\": HSL, \"HSR\": HSR, \"LL\": LL, \"LR\":LR, \"ML\": ML, \"MR\": MR, \"sentence\": label}\n",
    "\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((HSL_dataset, HSR_dataset, LL_dataset, LR_dataset, ML_dataset, MR_dataset, labels))\n",
    "# train_dataset = train_dataset.map(encode_dict, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "#                                  ).shuffle(100).batch(1).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_dataset = encode_dict(HSL_dataset, HSR_dataset, LL_dataset, LR_dataset, ML_dataset, MR_dataset, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"test_SLT\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "HSL (InputLayer)                [(None, None, 25, 10 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "HSR (InputLayer)                [(None, None, 25, 10 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "LL (InputLayer)                 [(None, None, 25, 12 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "LR (InputLayer)                 [(None, None, 25, 12 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ML (InputLayer)                 [(None, None, 1, 28) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "MR (InputLayer)                 [(None, None, 1, 28) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "HSLAttention (TimeDistributed)  (None, None, 25, 10) 100         HSL[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "HSRAttention (TimeDistributed)  (None, None, 25, 10) 100         HSR[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "LLAttention (TimeDistributed)   (None, None, 25, 12) 144         LL[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "LRAttention (TimeDistributed)   (None, None, 25, 12) 144         LR[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "MLAttention (TimeDistributed)   (None, None, 1, 28)  784         ML[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "MRAttention (TimeDistributed)   (None, None, 1, 28)  784         MR[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "HSLcontext (TimeDistributed)    (None, None, 23, 64) 1984        HSLAttention[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "HSRcontext (TimeDistributed)    (None, None, 23, 64) 1984        HSRAttention[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "LLcontext (TimeDistributed)     (None, None, 23, 64) 2368        LLAttention[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "LRcontext (TimeDistributed)     (None, None, 23, 64) 2368        LRAttention[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "MLcontext (TimeDistributed)     (None, None, 1, 1024 29696       MLAttention[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "MRcontext (TimeDistributed)     (None, None, 1, 1024 29696       MRAttention[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 1472)   0           HSLcontext[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 1472)   0           HSRcontext[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, None, 1472)   0           LLcontext[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, None, 1472)   0           LRcontext[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, None, 1024)   0           MLcontext[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, None, 1024)   0           MRcontext[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, 7936)   0           time_distributed[0][0]           \n",
      "                                                                 time_distributed_1[0][0]         \n",
      "                                                                 time_distributed_2[0][0]         \n",
      "                                                                 time_distributed_3[0][0]         \n",
      "                                                                 time_distributed_4[0][0]         \n",
      "                                                                 time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, None, 128)    4129280     concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, None, 128)    16512       lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "sentence (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Outputlayer (TimeDistributed)   (None, None, 26)     3354        time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "input.to_tensor (InstanceMethod (None, None)         0           sentence[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input.to_tensor_1 (InstanceMeth (None, None, 26)     0           Outputlayer[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "ctc-loss (CTCLayer)             (None, None, 26)     0           input.to_tensor[0][0]            \n",
      "                                                                 input.to_tensor_1[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 4,219,298\n",
      "Trainable params: 4,219,298\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer AttentionLayer has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minhyuklee\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/test_SLT/input.to_tensor_1/RaggedToTensor/boolean_mask_1/GatherV2:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/test_SLT/input.to_tensor_1/RaggedToTensor/boolean_mask/GatherV2:0\", shape=(None, 26), dtype=float32), dense_shape=Tensor(\"gradient_tape/test_SLT/input.to_tensor_1/RaggedToTensor/Shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 35s 198ms/step - loss: 15.9589\n",
      "Epoch 2/20\n",
      "90/90 [==============================] - 14s 154ms/step - loss: 15.1786\n",
      "Epoch 3/20\n",
      "90/90 [==============================] - 13s 141ms/step - loss: 14.3127\n",
      "Epoch 4/20\n",
      "90/90 [==============================] - 13s 142ms/step - loss: 12.3833\n",
      "Epoch 5/20\n",
      "90/90 [==============================] - 13s 144ms/step - loss: 8.7428\n",
      "Epoch 6/20\n",
      "90/90 [==============================] - 13s 146ms/step - loss: 6.7072\n",
      "Epoch 7/20\n",
      "90/90 [==============================] - 13s 149ms/step - loss: 5.3509\n",
      "Epoch 8/20\n",
      "90/90 [==============================] - 13s 143ms/step - loss: 4.9768\n",
      "Epoch 9/20\n",
      "90/90 [==============================] - 13s 143ms/step - loss: 4.3190\n",
      "Epoch 10/20\n",
      "90/90 [==============================] - 13s 143ms/step - loss: 3.9835\n",
      "Epoch 11/20\n",
      "90/90 [==============================] - 13s 144ms/step - loss: 4.2114\n",
      "Epoch 12/20\n",
      "90/90 [==============================] - 13s 149ms/step - loss: 4.4210\n",
      "Epoch 13/20\n",
      "90/90 [==============================] - 13s 146ms/step - loss: 4.2559\n",
      "Epoch 14/20\n",
      "90/90 [==============================] - 13s 142ms/step - loss: 4.2875\n",
      "Epoch 15/20\n",
      "90/90 [==============================] - 13s 139ms/step - loss: 3.9156\n",
      "Epoch 16/20\n",
      "90/90 [==============================] - 13s 145ms/step - loss: 4.1866\n",
      "Epoch 17/20\n",
      "90/90 [==============================] - 13s 149ms/step - loss: 3.5676\n",
      "Epoch 18/20\n",
      "90/90 [==============================] - 13s 140ms/step - loss: 3.9077\n",
      "Epoch 19/20\n",
      "90/90 [==============================] - 13s 141ms/step - loss: 3.8960\n",
      "Epoch 20/20\n",
      "90/90 [==============================] - 13s 142ms/step - loss: 4.6077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ab1af48848>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HS_dict = tf.convert_to_tensor(np.random.rand(10,4),dtype='float32')\n",
    "# L_dict = tf.convert_to_tensor(np.random.rand(12,3),dtype='float32')\n",
    "# M_dict = tf.convert_to_tensor(np.random.rand(window_size+3,3),dtype='float32')\n",
    "\n",
    "class CTCLayer(layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = keras.backend.ctc_batch_cost\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "#         y_pred = y_pred.to_tensor()\n",
    "        # compute the training-time loss value and add it\n",
    "        # to the layer using 'self.add_loss()'\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\") # shape의 [0] 차원은 batch_size를 의미함.\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\") # 매 timestep마다 output 출력하므로 y_pred의 길이가 input의 길이와 동일.      \n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\") # 실제 출력되어야 할 문장의 길이. 이 예시 경우엔 5. \n",
    "        \n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\") # input_length를 batch_size만큼 복제함.\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\") # label_length를 batch_size만큼 복제함.\n",
    "        \n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "        self.add_loss(loss)\n",
    "        \n",
    "        # At test time, just return the computed predictions\n",
    "        return y_pred\n",
    "    \n",
    "class AttentionLayer(keras.layers.Layer):\n",
    "    def __init__(self, phoneme_dict):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.phoneme_dict = phoneme_dict\n",
    "#         self.W1 = layers.Dense(units)\n",
    "#         self.W2 = layers.Dense(units)\n",
    "#         self.V = layers.Dense(1)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # 학습 가능한 가중치 변수 만들기\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "        self.kernel = self.add_weight(\"kernel\",\n",
    "                                     shape=[int(input_shape[-1]),\n",
    "                                           self.phoneme_dict.shape[0]])\n",
    "\n",
    "    def call(self, inputs): # HS_dict의 shape: (10,18)=(feature dimension, number of phonemes)\n",
    "        self.inputs = inputs\n",
    "        # BahdanauAttention\n",
    "#         a = self.V(tf.nn.tanh(\n",
    "#             self.W1(self.phoneme_dict) + self.W2(inputs)))\n",
    "#         print(a.shape)\n",
    "        \n",
    "        # General\n",
    "        tmp = tf.matmul(self.kernel, self.phoneme_dict)\n",
    "        score = tf.matmul(inputs,tmp)\n",
    "\n",
    "        # dot attention 사용 시\n",
    "#         score = tf.matmul(inputs, self.phoneme_dict)\n",
    "        split_timestep = tf.split(score, num_or_size_splits=inputs.shape[1], axis=1) # return = list of tensor\n",
    "        for i in range(len(split_timestep)):\n",
    "            alpha_each_time = tf.nn.softmax(split_timestep[i])\n",
    "            context_tmp = tf.matmul(tf.linalg.diag(alpha_each_time), tf.transpose(self.phoneme_dict))\n",
    "            context_each_time = tf.reduce_sum(context_tmp,2)\n",
    "            if i==0:\n",
    "                context_all_time = context_each_time\n",
    "            else:\n",
    "                context_all_time = tf.concat([context_all_time, context_each_time],1)\n",
    "        \n",
    "        return context_all_time\n",
    "    \n",
    "def build_model():\n",
    "    with tf.device('/gpu:0'):\n",
    "        # Inputs to the model\n",
    "        input_HSL = layers.Input(\n",
    "            shape=(None,window_size,10), name=\"HSL\", dtype=\"float32\", ragged=True)\n",
    "        \n",
    "        input_HSR = layers.Input(\n",
    "            shape=(None,window_size,10), name=\"HSR\", dtype=\"float32\", ragged=True)\n",
    "        \n",
    "        input_LL = layers.Input(\n",
    "            shape=(None,window_size,12), name=\"LL\", dtype=\"float32\", ragged=True)\n",
    "        \n",
    "        input_LR = layers.Input(\n",
    "            shape=(None,window_size,12), name=\"LR\", dtype=\"float32\", ragged=True)        \n",
    "        \n",
    "        input_ML = layers.Input(\n",
    "            shape=(None,1,window_size+3), name=\"ML\", dtype=\"float32\", ragged=True)        \n",
    "        \n",
    "        input_MR = layers.Input(\n",
    "            shape=(None,1,window_size+3), name=\"MR\", dtype=\"float32\", ragged=True)        \n",
    "        \n",
    "        labels = layers.Input(shape=(None,), name='sentence', dtype=\"float32\", ragged=True)\n",
    "\n",
    "        context_mat_HSL = layers.TimeDistributed(AttentionLayer(HS_dict), name=\"HSLAttention\")(input_HSL)\n",
    "        context_mat_HSR = layers.TimeDistributed(AttentionLayer(HS_dict), name=\"HSRAttention\")(input_HSR)\n",
    "        context_mat_LL = layers.TimeDistributed(AttentionLayer(L_dict), name=\"LLAttention\")(input_LL)\n",
    "        context_mat_LR = layers.TimeDistributed(AttentionLayer(L_dict), name=\"LRAttention\")(input_LR)\n",
    "        context_mat_ML = layers.TimeDistributed(AttentionLayer(M_dict), name=\"MLAttention\")(input_ML)\n",
    "        context_mat_MR = layers.TimeDistributed(AttentionLayer(M_dict), name=\"MRAttention\")(input_MR)      \n",
    "        \n",
    "#         context_mat_HSL = layers.Concatenate(axis=3)([context_mat_HSL,input_HSL])\n",
    "#         context_mat_HSR = layers.Concatenate(axis=3)([context_mat_HSR,input_HSR])\n",
    "#         context_mat_LL = layers.Concatenate(axis=3)([context_mat_LL,input_LL])\n",
    "#         context_mat_LR = layers.Concatenate(axis=3)([context_mat_LR,input_LR])\n",
    "#         context_mat_ML = layers.Concatenate(axis=3)([context_mat_ML,input_ML])\n",
    "#         context_mat_MR = layers.Concatenate(axis=3)([context_mat_MR,input_MR])\n",
    "\n",
    "        h_HSL = layers.TimeDistributed(layers.Conv1D(filters=64, kernel_size=3, strides=1, activation=\"relu\"), name=\"HSLcontext\")(context_mat_HSL)\n",
    "        h_HSL = layers.TimeDistributed(layers.Flatten())(h_HSL)\n",
    "        \n",
    "        h_HSR = layers.TimeDistributed(layers.Conv1D(filters=64, kernel_size=3, strides=1, activation=\"relu\"), name=\"HSRcontext\")(context_mat_HSR)\n",
    "        h_HSR = layers.TimeDistributed(layers.Flatten())(h_HSR)\n",
    "        \n",
    "        h_LL = layers.TimeDistributed(layers.Conv1D(filters=64, kernel_size=3, strides=1, activation=\"relu\"), name=\"LLcontext\")(context_mat_LL)\n",
    "        h_LL = layers.TimeDistributed(layers.Flatten())(h_LL)\n",
    "        \n",
    "        h_LR = layers.TimeDistributed(layers.Conv1D(filters=64, kernel_size=3, strides=1, activation=\"relu\"), name=\"LRcontext\")(context_mat_LR)\n",
    "        h_LR = layers.TimeDistributed(layers.Flatten())(h_LR)\n",
    "        \n",
    "        h_ML = layers.TimeDistributed(layers.Dense(1024, activation=\"relu\"), name=\"MLcontext\")(context_mat_ML)\n",
    "        h_ML = layers.TimeDistributed(layers.Flatten())(h_ML)\n",
    "        \n",
    "        h_MR = layers.TimeDistributed(layers.Dense(1024, activation=\"relu\"), name=\"MRcontext\")(context_mat_MR)\n",
    "        h_MR = layers.TimeDistributed(layers.Flatten())(h_MR)\n",
    "        \n",
    "        concat = layers.Concatenate(axis=2)([h_HSL,h_HSR,h_LL,h_LR,h_ML,h_MR])\n",
    "#         h = layers.TimeDistributed(layers.Dense(512, activation=\"relu\"))(concat)\n",
    "#         h = layers.TimeDistributed(layers.Dense(256, activation=\"relu\"))(h)\n",
    "#         h = layers.TimeDistributed(layers.Dense(128, activation=\"relu\"))(h)\n",
    "        \n",
    "#         h = layers.LSTM(128, return_sequences=True)(h)\n",
    "        h = layers.LSTM(128, return_sequences=True)(concat)\n",
    "#         h = layers.LSTM(128, return_sequences=True)(h)\n",
    "        h = layers.TimeDistributed(layers.Dense(128, activation=\"relu\"))(h)\n",
    "        h = layers.TimeDistributed(layers.Dense(num_words+1, activation=\"softmax\"), name=\"Outputlayer\")(h)\n",
    "        output = CTCLayer(name=\"ctc-loss\")(labels.to_tensor(), h.to_tensor())\n",
    "\n",
    "        model = keras.models.Model(\n",
    "            inputs=[input_HSL, input_HSR, input_LL, input_LR, input_ML, input_MR, labels], outputs=output, name=\"test_SLT\"\n",
    "        )\n",
    "        opt = keras.optimizers.Adam()\n",
    "        \n",
    "        # Compile the model and return\n",
    "        model.compile(optimizer=opt)\n",
    "        return model    \n",
    "    \n",
    "        # Handshape Attention block\n",
    "model = build_model()\n",
    "model.summary()\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(train_dataset, batch_size=2, epochs=20, callbacks=tensorboard_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minhyuklee\\Anaconda3\\envs\\research\\lib\\site-packages\\tensorflow\\python\\ops\\ragged\\ragged_tensor.py:2063: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\n",
      "[[ 0  1  2 -1 -1]\n",
      " [ 4  2 -1 -1 -1]\n",
      " [ 0  7 23 22  5]\n",
      " [10  8 11 -1 -1]\n",
      " [ 4  2 -1 -1 -1]\n",
      " [ 0  7 23 22  5]\n",
      " [ 4  2 -1 -1 -1]\n",
      " [ 8  5  9 -1 -1]\n",
      " [15  1 22  5 -1]\n",
      " [15  8  6 21 -1]\n",
      " [10  8 23 24 21]\n",
      " [ 3  4  5 -1 -1]\n",
      " [ 6  7  2 -1 -1]\n",
      " [ 4  2  9 -1 -1]\n",
      " [ 3 13  1  5 -1]\n",
      " [15  8  6 21 -1]\n",
      " [ 8  5  9 -1 -1]\n",
      " [10  7  0 18 -1]\n",
      " [15  1 22  5 -1]\n",
      " [12 13  8  5 -1]\n",
      " [ 0 14 15 16 -1]\n",
      " [10  8 11 -1 -1]\n",
      " [ 6  7  2 -1 -1]\n",
      " [ 0  7 23 22  5]\n",
      " [ 0  1  2 -1 -1]\n",
      " [10  7  0 18 -1]\n",
      " [11 14 23 24 20]\n",
      " [11 14 23 24 20]\n",
      " [10  8 23 24 21]\n",
      " [15  1 22  5 -1]\n",
      " [11 14 23 24 20]\n",
      " [ 0  1  2 -1 -1]\n",
      " [11 14 23 24 20]\n",
      " [10  8 11 -1 -1]\n",
      " [10  7  0 18 -1]\n",
      " [19 13  1 20 -1]\n",
      " [ 8  5 -1 -1 -1]\n",
      " [ 3 13  1  5 -1]\n",
      " [15  8  6 21 -1]\n",
      " [15  1 22  5 -1]\n",
      " [ 0 13 17 18 -1]\n",
      " [10  8 11 -1 -1]\n",
      " [12 13  8  5 -1]\n",
      " [10  8 11 -1 -1]\n",
      " [12 13  8  5 -1]\n",
      " [12 13  8  5 -1]\n",
      " [ 3  4  5 -1 -1]\n",
      " [ 4  2 -1 -1 -1]\n",
      " [ 0 14 15 16 -1]\n",
      " [ 0 14 15 16 -1]\n",
      " [19 13  1 20 -1]\n",
      " [ 6  7  2 -1 -1]\n",
      " [15  8  6 21 -1]\n",
      " [ 3 13  1  5 -1]\n",
      " [ 3 13  1  5 -1]\n",
      " [15  1 22  5 -1]\n",
      " [ 6  7  2 -1 -1]\n",
      " [ 0 14 15 16 -1]\n",
      " [ 3 13  1  5 -1]\n",
      " [ 0 13 17 18 -1]\n",
      " [ 8  5  9 -1 -1]\n",
      " [ 0  1  2 -1 -1]\n",
      " [15  8  6 21 -1]\n",
      " [ 0  7 23 22 -1]\n",
      " [ 3  4 -1 -1 -1]\n",
      " [ 6  7  2 -1 -1]\n",
      " [ 3 13  1  5 -1]\n",
      " [ 8  5  9 -1 -1]\n",
      " [19 13  1 20 -1]\n",
      " [ 0  7 23 22 -1]\n",
      " [11 14 23 24 20]\n",
      " [15  7 23 24 21]\n",
      " [10  8 23 24 21]\n",
      " [10  7  0 18 -1]\n",
      " [15  7 23 24 21]\n",
      " [ 0  7 23 22 -1]\n",
      " [19 13  1 20 -1]\n",
      " [ 0  7 23 22  5]\n",
      " [10  8 11 -1 -1]\n",
      " [ 0 14 15 16 -1]\n",
      " [12 13  8  5 -1]\n",
      " [ 0 13 17 18 -1]\n",
      " [10  7  0 18 -1]\n",
      " [12 13  8  5 -1]\n",
      " [15  1 22  5 -1]\n",
      " [15  7 23 24 21]\n",
      " [ 0 13 17 18 -1]\n",
      " [10  8 11 -1 -1]\n",
      " [ 0 14 15 16 -1]\n",
      " [15  7 23 24 21]\n",
      " [10  7  0 18 -1]\n",
      " [10  8 23 24 21]\n",
      " [10  8 11 -1 -1]\n",
      " [ 8  5  9 -1 -1]\n",
      " [ 6  7  2 -1 -1]\n",
      " [ 3  4  5 -1 -1]\n",
      " [15  7 23 24 21]\n",
      " [10  8 23 24 21]\n",
      " [11 14 23 24 20]\n",
      " [19 13  1 20 -1]\n",
      " [ 4  2  9 -1 -1]\n",
      " [11 14 23 24 20]\n",
      " [ 6  7  2 -1 -1]\n",
      " [11 14 23 24 20]\n",
      " [ 0 14 15 16 -1]\n",
      " [ 0  7 23 22  5]\n",
      " [11 14 23 24 20]\n",
      " [ 3  4  5 -1 -1]\n",
      " [15  7 23 24 21]\n",
      " [ 3 13  1  5 -1]\n",
      " [15  7 23 24 21]\n",
      " [ 6  7  2 -1 -1]\n",
      " [ 3  4  5 -1 -1]\n",
      " [10  7  0 18 -1]\n",
      " [15  1 22  5 -1]\n",
      " [ 0  7 23 22  5]\n",
      " [19 13  1 20 -1]\n",
      " [15  7 23 24 21]\n",
      " [10  7  0 18 -1]\n",
      " [19 13  1 20 -1]\n",
      " [12 13  8  5 -1]\n",
      " [12 13  8  5 -1]\n",
      " [ 3  4  5 -1 -1]\n",
      " [ 0  1 -1 -1 -1]\n",
      " [10  8 11 -1 -1]\n",
      " [ 3  4  5 -1 -1]\n",
      " [ 8  5  9 -1 -1]\n",
      " [15  8  6 21 -1]\n",
      " [10  8 23 24 21]\n",
      " [10  7  0 18 -1]\n",
      " [12 13  8  5 -1]\n",
      " [ 4  2 -1 -1 -1]\n",
      " [10  8 23 24 21]\n",
      " [10  7  0 18 -1]\n",
      " [ 0  1  2 -1 -1]\n",
      " [ 4  2 -1 -1 -1]\n",
      " [ 3  4  5 -1 -1]\n",
      " [ 0 13 17 18 -1]\n",
      " [ 4  2  9 -1 -1]\n",
      " [ 3 13  1  5 -1]\n",
      " [ 0 13 17 18 -1]\n",
      " [ 8  5  9 -1 -1]\n",
      " [10  8 23 24 21]\n",
      " [ 0  1  2 -1 -1]\n",
      " [ 0 13 17 18 -1]\n",
      " [10  8 11 -1 -1]\n",
      " [ 4  2 -1 -1 -1]\n",
      " [ 3  4  5 -1 -1]\n",
      " [ 0 14 15 16 -1]\n",
      " [ 6  7  2 -1 -1]\n",
      " [ 6  7  2 -1 -1]\n",
      " [15  8  6 21 -1]\n",
      " [ 0 13 17 18 -1]\n",
      " [12 13  8  5 -1]\n",
      " [ 0 14 15 16 -1]\n",
      " [11 14 23 24 20]\n",
      " [ 0  1  2 -1 -1]\n",
      " [19 13 -1 -1 -1]\n",
      " [ 0 13 17 18 -1]\n",
      " [ 0  1  2 -1 -1]\n",
      " [ 0 13 17 18 -1]\n",
      " [ 3 13  1  5 -1]\n",
      " [ 8  5 -1 -1 -1]\n",
      " [19 13  1 20 -1]\n",
      " [15  1 22  5 -1]\n",
      " [ 0  7 23 22 -1]\n",
      " [ 3 13  1  5 -1]\n",
      " [10  8 23 24 21]\n",
      " [15  1 22  5 -1]\n",
      " [ 0  1  2 -1 -1]\n",
      " [15  8  6 21 -1]\n",
      " [15  1 22  5 -1]\n",
      " [15  7 23 24 21]\n",
      " [15  8  6 21 -1]\n",
      " [ 0 14 15 16 -1]\n",
      " [ 8  5  9 -1 -1]\n",
      " [15  8  6 21 -1]\n",
      " [19 13  1 20 -1]\n",
      " [10  8 23 24 21]\n",
      " [15  7 23 24 21]]\n",
      "Ground truth\n",
      "[array([0, 1, 2]), array([4, 2, 9]), array([ 0,  7, 23, 22,  5]), array([10,  8, 11]), array([4, 2, 9]), array([ 0,  7, 23, 22,  5]), array([4, 2, 9]), array([8, 5, 9]), array([15,  1, 22,  5]), array([15,  8,  6, 21]), array([10,  8, 23, 24, 21]), array([3, 4, 5]), array([6, 7, 2]), array([4, 2, 9]), array([ 3, 13,  1,  5]), array([15,  8,  6, 21]), array([8, 5, 9]), array([10,  7,  0, 18]), array([15,  1, 22,  5]), array([12, 13,  8,  5]), array([ 0, 14, 15, 16]), array([10,  8, 11]), array([6, 7, 2]), array([ 0,  7, 23, 22,  5]), array([0, 1, 2]), array([10,  7,  0, 18]), array([11, 14, 23, 24, 20]), array([11, 14, 23, 24, 20]), array([10,  8, 23, 24, 21]), array([15,  1, 22,  5]), array([11, 14, 23, 24, 20]), array([0, 1, 2]), array([11, 14, 23, 24, 20]), array([10,  8, 11]), array([10,  7,  0, 18]), array([19, 13,  1, 20]), array([8, 5, 9]), array([ 3, 13,  1,  5]), array([15,  8,  6, 21]), array([15,  1, 22,  5]), array([ 0, 13, 17, 18]), array([10,  8, 11]), array([12, 13,  8,  5]), array([10,  8, 11]), array([12, 13,  8,  5]), array([12, 13,  8,  5]), array([3, 4, 5]), array([4, 2, 9]), array([ 0, 14, 15, 16]), array([ 0, 14, 15, 16]), array([19, 13,  1, 20]), array([6, 7, 2]), array([15,  8,  6, 21]), array([ 3, 13,  1,  5]), array([ 3, 13,  1,  5]), array([15,  1, 22,  5]), array([6, 7, 2]), array([ 0, 14, 15, 16]), array([ 3, 13,  1,  5]), array([ 0, 13, 17, 18]), array([8, 5, 9]), array([0, 1, 2]), array([15,  8,  6, 21]), array([ 0,  7, 23, 22,  5]), array([3, 4, 5]), array([6, 7, 2]), array([ 3, 13,  1,  5]), array([8, 5, 9]), array([19, 13,  1, 20]), array([ 0,  7, 23, 22,  5]), array([11, 14, 23, 24, 20]), array([15,  7, 23, 24, 21]), array([10,  8, 23, 24, 21]), array([10,  7,  0, 18]), array([15,  7, 23, 24, 21]), array([ 0,  7, 23, 22,  5]), array([19, 13,  1, 20]), array([ 0,  7, 23, 22,  5]), array([10,  8, 11]), array([ 0, 14, 15, 16]), array([12, 13,  8,  5]), array([ 0, 13, 17, 18]), array([10,  7,  0, 18]), array([12, 13,  8,  5]), array([15,  1, 22,  5]), array([15,  7, 23, 24, 21]), array([ 0, 13, 17, 18]), array([10,  8, 11]), array([ 0, 14, 15, 16]), array([15,  7, 23, 24, 21]), array([10,  7,  0, 18]), array([10,  8, 23, 24, 21]), array([10,  8, 11]), array([8, 5, 9]), array([6, 7, 2]), array([3, 4, 5]), array([15,  7, 23, 24, 21]), array([10,  8, 23, 24, 21]), array([11, 14, 23, 24, 20]), array([19, 13,  1, 20]), array([4, 2, 9]), array([11, 14, 23, 24, 20]), array([6, 7, 2]), array([11, 14, 23, 24, 20]), array([ 0, 14, 15, 16]), array([ 0,  7, 23, 22,  5]), array([11, 14, 23, 24, 20]), array([3, 4, 5]), array([15,  7, 23, 24, 21]), array([ 3, 13,  1,  5]), array([15,  7, 23, 24, 21]), array([6, 7, 2]), array([3, 4, 5]), array([10,  7,  0, 18]), array([15,  1, 22,  5]), array([ 0,  7, 23, 22,  5]), array([19, 13,  1, 20]), array([15,  7, 23, 24, 21]), array([10,  7,  0, 18]), array([19, 13,  1, 20]), array([12, 13,  8,  5]), array([12, 13,  8,  5]), array([3, 4, 5]), array([0, 1, 2]), array([10,  8, 11]), array([3, 4, 5]), array([8, 5, 9]), array([15,  8,  6, 21]), array([10,  8, 23, 24, 21]), array([10,  7,  0, 18]), array([12, 13,  8,  5]), array([4, 2, 9]), array([10,  8, 23, 24, 21]), array([10,  7,  0, 18]), array([0, 1, 2]), array([4, 2, 9]), array([3, 4, 5]), array([ 0, 13, 17, 18]), array([4, 2, 9]), array([ 3, 13,  1,  5]), array([ 0, 13, 17, 18]), array([8, 5, 9]), array([10,  8, 23, 24, 21]), array([0, 1, 2]), array([ 0, 13, 17, 18]), array([10,  8, 11]), array([4, 2, 9]), array([3, 4, 5]), array([ 0, 14, 15, 16]), array([6, 7, 2]), array([6, 7, 2]), array([15,  8,  6, 21]), array([ 0, 13, 17, 18]), array([12, 13,  8,  5]), array([ 0, 14, 15, 16]), array([11, 14, 23, 24, 20]), array([0, 1, 2]), array([19, 13,  1, 20]), array([ 0, 13, 17, 18]), array([0, 1, 2]), array([ 0, 13, 17, 18]), array([ 3, 13,  1,  5]), array([8, 5, 9]), array([19, 13,  1, 20]), array([15,  1, 22,  5]), array([ 0,  7, 23, 22,  5]), array([ 3, 13,  1,  5]), array([10,  8, 23, 24, 21]), array([15,  1, 22,  5]), array([0, 1, 2]), array([15,  8,  6, 21]), array([15,  1, 22,  5]), array([15,  7, 23, 24, 21]), array([15,  8,  6, 21]), array([ 0, 14, 15, 16]), array([8, 5, 9]), array([15,  8,  6, 21]), array([19, 13,  1, 20]), array([10,  8, 23, 24, 21]), array([15,  7, 23, 24, 21])]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Test dataset load\n",
    "# Get list of all the sentences\n",
    "HSL_test_path = r\"D:\\\\MinHyuk\\\\Hand Sign Recognition\\\\2021_SLT_prj\\\\DATA\\\\feature\\\\HSL\\\\testset\"\n",
    "HSR_test_path = r\"D:\\\\MinHyuk\\\\Hand Sign Recognition\\\\2021_SLT_prj\\\\DATA\\\\feature\\\\HSR\\\\testset\"\n",
    "LL_test_path = r\"D:\\\\MinHyuk\\\\Hand Sign Recognition\\\\2021_SLT_prj\\\\DATA\\\\feature\\\\LL\\\\testset\"\n",
    "LR_test_path = r\"D:\\\\MinHyuk\\\\Hand Sign Recognition\\\\2021_SLT_prj\\\\DATA\\\\feature\\\\LR\\\\testset\"\n",
    "ML_test_path = r\"D:\\\\MinHyuk\\\\Hand Sign Recognition\\\\2021_SLT_prj\\\\DATA\\\\feature\\\\ML\\\\testset\"\n",
    "MR_test_path = r\"D:\\\\MinHyuk\\\\Hand Sign Recognition\\\\2021_SLT_prj\\\\DATA\\\\feature\\\\MR\\\\testset\"\n",
    "\n",
    "HSL_test_all_files = glob(HSL_test_path+\"\\\\*.csv\")\n",
    "HSR_test_all_files = glob(HSR_test_path+\"\\\\*.csv\")\n",
    "LL_test_all_files = glob(LL_test_path+\"\\\\*.csv\")\n",
    "LR_test_all_files = glob(LR_test_path+\"\\\\*.csv\")\n",
    "ML_test_all_files = glob(ML_test_path+\"\\\\*.csv\")\n",
    "MR_test_all_files = glob(MR_test_path+\"\\\\*.csv\")\n",
    "\n",
    "HSL_test_sentences = cubic_sentences(HSL_test_all_files)\n",
    "HSR_test_sentences = cubic_sentences(HSR_test_all_files)\n",
    "LL_test_sentences = cubic_sentences(LL_test_all_files)\n",
    "LR_test_sentences = cubic_sentences(LR_test_all_files)\n",
    "ML_test_sentences = cubic_sentences(ML_test_all_files)\n",
    "MR_test_sentences = cubic_sentences(MR_test_all_files)\n",
    "\n",
    "stc_seq = [[0,1,2],[3,4,5],[6,7,2],[8,5,9],[10,8,11],[4,2,9],[12,13,8,5],\n",
    "          [0,14,15,16],[0,13,17,18],[19,13,1,20],[3,13,1,5],[15,8,6,21],\n",
    "          [15,1,22,5],[10,7,0,18],[11,14,23,24,20],[0,7,23,22,5],[15,7,23,24,21],\n",
    "          [10,8,23,24,21]] # 0부터 인덱싱\n",
    "num_words = 25\n",
    "labels = []\n",
    "for filename in HSL_all_files:\n",
    "    temp = filename.split(os.path.sep)[-1].split(\".csv\")[0]\n",
    "    temp = temp.split('-')[1]\n",
    "    temp = temp[-1]\n",
    "    labels.append(np.array(stc_seq[int(temp)-1]))\n",
    "labels_test = tf.ragged.constant(labels)\n",
    "\n",
    "HSL_test_dataset = tf.ragged.constant(HSL_test_sentences, inner_shape=(window_size,10))\n",
    "HSR_test_dataset = tf.ragged.constant(HSR_test_sentences, inner_shape=(window_size,10))\n",
    "LL_test_dataset = tf.ragged.constant(LL_test_sentences, inner_shape=(window_size,12))\n",
    "LR_test_dataset = tf.ragged.constant(LR_test_sentences, inner_shape=(window_size,12))\n",
    "ML_test_dataset = tf.ragged.constant(ML_test_sentences, inner_shape=(1,window_size+3))\n",
    "MR_test_dataset = tf.ragged.constant(MR_test_sentences, inner_shape=(1,window_size+3))\n",
    "\n",
    "indices = tf.range(start=0, limit=len(HSL_test_sentences))\n",
    "shuffled_indices = tf.random.shuffle(indices)\n",
    "\n",
    "HSL_test_dataset = tf.gather(HSL_test_dataset, shuffled_indices)\n",
    "HSR_test_dataset = tf.gather(HSR_test_dataset, shuffled_indices)\n",
    "LL_test_dataset = tf.gather(LL_test_dataset, shuffled_indices)\n",
    "LR_test_dataset = tf.gather(LR_test_dataset, shuffled_indices)\n",
    "ML_test_dataset = tf.gather(ML_test_dataset, shuffled_indices)\n",
    "MR_test_dataset = tf.gather(MR_test_dataset, shuffled_indices)\n",
    "labels_test = tf.gather(labels_test, shuffled_indices)\n",
    "\n",
    "test_dataset = encode_dict(HSL_test_dataset, HSR_test_dataset, LL_test_dataset, LR_test_dataset, ML_test_dataset, MR_test_dataset, labels_test)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "prediction_model = keras.models.Model(\n",
    "    inputs=[model.get_layer(name=\"HSL\").input, model.get_layer(name=\"HSR\").input, model.get_layer(name=\"LL\").input, \n",
    "            model.get_layer(name=\"LR\").input, model.get_layer(name=\"ML\").input, model.get_layer(name=\"MR\").input], \n",
    "    outputs=model.get_layer(name=\"Outputlayer\").output\n",
    "\n",
    ")\n",
    "# prediction_model.summary()\n",
    "\n",
    "def decode_batch_predictions(pred):\n",
    "    a = pred.numpy()\n",
    "    input_len = []\n",
    "    for i in range(len(a)):\n",
    "        input_len = np.append(input_len, a[i].shape[0])\n",
    "#     print(input_len)\n",
    "    \n",
    "    \n",
    "#     input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    results = keras.backend.ctc_decode(pred.to_tensor(), input_length=input_len, greedy=True)[0][0][:, :5]\n",
    "    \n",
    "    return results\n",
    "\n",
    "save_dir = \"D:\\\\MinHyuk\\\\Hand Sign Recognition\\\\2021_SLT_prj\\\\Analysis\"\n",
    "\n",
    "\n",
    "batch_HSL = train_dataset.get(\"HSL\")\n",
    "batch_HSR = train_dataset.get(\"HSR\")\n",
    "batch_LL = train_dataset.get(\"LL\")\n",
    "batch_LR = train_dataset.get(\"LR\")\n",
    "batch_ML = train_dataset.get(\"ML\")\n",
    "batch_MR = train_dataset.get(\"MR\")\n",
    "batch_labels = train_dataset.get(\"sentence\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "batch_HSL = test_dataset.get(\"HSL\")\n",
    "batch_HSR = test_dataset.get(\"HSR\")\n",
    "batch_LL = test_dataset.get(\"LL\")\n",
    "batch_LR = test_dataset.get(\"LR\")\n",
    "batch_ML = test_dataset.get(\"ML\")\n",
    "batch_MR = test_dataset.get(\"MR\")\n",
    "batch_labels = test_dataset.get(\"sentence\")\n",
    "\"\"\"\n",
    "\n",
    "preds = prediction_model.predict([batch_HSL, batch_HSR, batch_LL, batch_LR, batch_ML, batch_MR])\n",
    "pred_sentences = decode_batch_predictions(preds)\n",
    "pred_sentences = pred_sentences.numpy()\n",
    "\n",
    "for i in range(preds.shape[0]):\n",
    "    dataframe1 = pd.DataFrame(preds[i].numpy())\n",
    "    dataframe1.to_csv(save_dir+\"\\\\prediction\"+str(i)+\".csv\", header=False, index=False)\n",
    "    \n",
    "    dataframe2 = pd.DataFrame(pred_sentences[i])\n",
    "    dataframe2.to_csv(save_dir+\"\\\\pred_decoded\"+str(i)+\".csv\", header=False, index=False)\n",
    "    \n",
    "    dataframe3 = pd.DataFrame(batch_labels[i].numpy())\n",
    "    dataframe3.to_csv(save_dir+\"\\\\true\"+str(i)+\".csv\", header=False, index=False)\n",
    "\n",
    "print('Prediction')\n",
    "print(pred_sentences)\n",
    "\n",
    "orig_labels = []\n",
    "for label in batch_labels:\n",
    "    label = label.numpy()\n",
    "    orig_labels.append(label)\n",
    "\n",
    "print('Ground truth')\n",
    "print(orig_labels)\n",
    "#    for i in range(len(pred_gestures)):\n",
    "#        plt.figure(figsize=(20,5))\n",
    "#        plt.subplot(3,2,i+1)\n",
    "#        plt.plot(batch_sentences[i,:,:].numpy())\n",
    "#        title = f\"Prediction: {pred_gestures[i]}\"\n",
    "#        plt.title(title)\n",
    "\n",
    "#print(pred_gestures)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[[1,2,3],[4,5,6]], [[4,5,6],[7,8,9]]])\n",
    "b = tf.constant([[1,2],[3,4],[5,6]])\n",
    "c = tf.matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
